{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d5d0cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T15:27:53.745513Z",
     "iopub.status.busy": "2025-06-09T15:27:53.744713Z",
     "iopub.status.idle": "2025-06-09T15:51:22.185178Z",
     "shell.execute_reply": "2025-06-09T15:51:22.184145Z"
    },
    "id": "mGWuWq4vzyDg",
    "papermill": {
     "duration": 1408.449988,
     "end_time": "2025-06-09T15:51:22.187111",
     "exception": false,
     "start_time": "2025-06-09T15:27:53.737123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\r\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78f0625e2910>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pip/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78f0625cb010>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pip/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78f0626cd490>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pip/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78f0625b0350>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pip/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78f0625dd110>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pip/\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\r\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\r\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\r\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e868d25d8d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cudnn-cu12/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e868d25e490>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cudnn-cu12/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e868d25eb10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cudnn-cu12/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e868d25f510>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cudnn-cu12/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e868d26c310>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cudnn-cu12/\u001b[0m\u001b[33m\r\n",
      "\u001b[0mINFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\r\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e868d254210>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e868d416210>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e868d2504d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e868d252690>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7e868d25e910>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch) (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\"\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7bce2594a990>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/faiss-cpu/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7bce25705ad0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/faiss-cpu/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7bce25961050>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/faiss-cpu/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7bce257f2190>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/faiss-cpu/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7bce257ac790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/faiss-cpu/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-cpu (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-cpu\u001b[0m\u001b[31m\r\n",
      "\u001b[0mRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.1)\r\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\r\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dcd588a2f10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cudnn-cu12/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dcd588cff10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cudnn-cu12/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dcd588d85d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cudnn-cu12/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dcd588d9250>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cudnn-cu12/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dcd588d9d10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/nvidia-cudnn-cu12/\u001b[0m\u001b[33m\r\n",
      "\u001b[0mINFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\r\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dcd588c6a10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dcd588c6a50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dcd588b8610>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dcd588bfd90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dcd588c0550>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/torch/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dcd588affd0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sentence-transformers/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dcd588a9490>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sentence-transformers/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dcd588c6a50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sentence-transformers/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dcd588c0fd0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sentence-transformers/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7dcd588c1310>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sentence-transformers/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" (from torch) (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\"\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install transformers torch sentencepiece accelerate\n",
    "!pip install faiss-cpu --quiet\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39cae07",
   "metadata": {
    "papermill": {
     "duration": 0.007013,
     "end_time": "2025-06-09T15:51:22.201878",
     "exception": false,
     "start_time": "2025-06-09T15:51:22.194865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Login in HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2445562b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T15:51:22.217776Z",
     "iopub.status.busy": "2025-06-09T15:51:22.217408Z",
     "iopub.status.idle": "2025-06-09T15:51:55.028831Z",
     "shell.execute_reply": "2025-06-09T15:51:55.027050Z"
    },
    "papermill": {
     "duration": 32.828522,
     "end_time": "2025-06-09T15:51:55.037582",
     "exception": true,
     "start_time": "2025-06-09T15:51:22.209060",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /api/whoami-v2 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7d7d3df0ac50>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 36d70a4b-d45f-43bf-900d-560cea2ff8c7)')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    199\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_proxy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLSocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaierror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNameResolutionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7d7d3df0ac50>: Failed to resolve 'huggingface.co' ([Errno -3] Temporary failure in name resolution)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    842\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreason\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/whoami-v2 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7d7d3df0ac50>: Failed to resolve 'huggingface.co' ([Errno -3] Temporary failure in name resolution)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13/4078110699.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"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\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcustom_message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             args_msg = [\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/_login.py\u001b[0m in \u001b[0;36mlogin\u001b[0;34m(token, add_to_git_credential, new_session, write_permission)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;34m\"you want to set the git credential as well.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             )\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0m_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_to_git_credential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_to_git_credential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mnotebook_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(token, add_to_git_credential)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must use your personal account token, not an organization token.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m     \u001b[0mtoken_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhoami\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0mpermission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auth\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accessToken\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Token is valid (permission: {permission}).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mwhoami\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m   1739\u001b[0m         \u001b[0;31m# Get the effective token using the helper function get_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1740\u001b[0m         \u001b[0meffective_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mget_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1741\u001b[0;31m         r = get_session().get(\n\u001b[0m\u001b[1;32m   1742\u001b[0m             \u001b[0;34mf\"{self.endpoint}/api/whoami-v2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_hf_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meffective_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allow_redirects\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Send: {_curlify(request)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_AMZN_TRACE_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /api/whoami-v2 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7d7d3df0ac50>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 36d70a4b-d45f-43bf-900d-560cea2ff8c7)')"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2640ee8b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Import Neccessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8347f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:47:38.970764Z",
     "iopub.status.busy": "2025-05-27T16:47:38.970505Z",
     "iopub.status.idle": "2025-05-27T16:48:04.750680Z",
     "shell.execute_reply": "2025-05-27T16:48:04.749956Z",
     "shell.execute_reply.started": "2025-05-27T16:47:38.970741Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import faiss # pip install faiss-cpu (or faiss-gpu)\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer # pip install sentence-transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM # pip install transformers torch sentencepiece accelerate\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a37595",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Upload Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99edac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:48:04.753287Z",
     "iopub.status.busy": "2025-05-27T16:48:04.752494Z",
     "iopub.status.idle": "2025-05-27T16:48:04.756980Z",
     "shell.execute_reply": "2025-05-27T16:48:04.756183Z",
     "shell.execute_reply.started": "2025-05-27T16:48:04.753264Z"
    },
    "id": "PRVG4pDgzwNH",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "FAISS_INDEX_PATH = \"/kaggle/input/knowledge-data/drilling_knowledge.index\"\n",
    "CHUNKS_JSON_PATH = \"/kaggle/input/knowledge-data/drilling_knowledge_chunks.json\"\n",
    "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'     # MUST match the model used for indexing\n",
    "LLM_PROVIDER = \"huggingface_local\" # Explicitly set for this script\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddaedb0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Choose A Specific Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5fac1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:48:04.758615Z",
     "iopub.status.busy": "2025-05-27T16:48:04.757724Z",
     "iopub.status.idle": "2025-05-27T16:48:04.778505Z",
     "shell.execute_reply": "2025-05-27T16:48:04.777818Z",
     "shell.execute_reply.started": "2025-05-27T16:48:04.758586Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Hugging Face Local Model Configuration ---\n",
    "# LOCAL_LLM_MODEL_NAME = \"NousResearch/Llama-2-7b-chat-hf\" # ~14GB VRAM for float16\n",
    "# LOCAL_LLM_MODEL_NAME = \"google/flan-t5-large\" # ~3GB VRAM for float16, or more RAM if on CPU. Better for smaller setups but may struggle with complex JSON.\n",
    "# LOCAL_LLM_MODEL_NAME = \"deepset/roberta-base-squad2\"\n",
    "# LOCAL_LLM_MODEL_NAME = \"deepseek-ai/DeepSeek-R1\"\n",
    "LOCAL_LLM_MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Global variable for the loaded model and tokenizer to avoid reloading on every call\n",
    "local_llm_pipeline_instance = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c969c0b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# --- 1. Load Indexed Knowledge ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c922f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:48:04.779624Z",
     "iopub.status.busy": "2025-05-27T16:48:04.779368Z",
     "iopub.status.idle": "2025-05-27T16:48:04.790323Z",
     "shell.execute_reply": "2025-05-27T16:48:04.789801Z",
     "shell.execute_reply.started": "2025-05-27T16:48:04.779599Z"
    },
    "id": "ZEtBHrojz26Z",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. Load Indexed Knowledge ---\n",
    "def load_indexed_knowledge(index_path, chunks_path):\n",
    "    \"\"\"Loads the FAISS index and the corresponding text chunks.\"\"\"\n",
    "    print(f\"Loading FAISS index from: {index_path}\")\n",
    "    if not os.path.exists(index_path):\n",
    "        print(f\"Error: FAISS index file not found at {index_path}\")\n",
    "        return None, None\n",
    "    try:\n",
    "        index = faiss.read_index(index_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading FAISS index: {e}. Make sure the file exists and was created correctly.\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\"Loading knowledge chunks from: {chunks_path}\")\n",
    "    if not os.path.exists(chunks_path):\n",
    "        print(f\"Error: Chunks JSON file not found at {chunks_path}\")\n",
    "        return index, None\n",
    "    try:\n",
    "        with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "            knowledge_chunks_with_metadata = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading knowledge chunks: {e}. Make sure the file exists and is valid JSON.\")\n",
    "        return index, None\n",
    "\n",
    "    print(f\"Successfully loaded FAISS index with {index.ntotal} vectors and {len(knowledge_chunks_with_metadata)} text chunks.\")\n",
    "    return index, knowledge_chunks_with_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439ffcc8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# --- 2. Initialize Embedding Model (for querying) ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993af6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:48:04.791334Z",
     "iopub.status.busy": "2025-05-27T16:48:04.791170Z",
     "iopub.status.idle": "2025-05-27T16:48:04.804470Z",
     "shell.execute_reply": "2025-05-27T16:48:04.803946Z",
     "shell.execute_reply.started": "2025-05-27T16:48:04.791320Z"
    },
    "id": "Gkn1AjTK0Hdq",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 2. Initialize Embedding Model (for querying) ---\n",
    "def initialize_embedding_model(model_name):\n",
    "    \"\"\"Initializes the sentence transformer model for embedding queries.\"\"\"\n",
    "    print(f\"Initializing query embedding model: {model_name}\")\n",
    "    try:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing SentenceTransformer model '{model_name}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01124f8c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# --- 3. Retrieve Relevant Knowledge ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b835ea5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T16:48:04.805427Z",
     "iopub.status.busy": "2025-05-27T16:48:04.805186Z",
     "iopub.status.idle": "2025-05-27T16:48:04.817427Z",
     "shell.execute_reply": "2025-05-27T16:48:04.816814Z",
     "shell.execute_reply.started": "2025-05-27T16:48:04.805405Z"
    },
    "id": "QmDuo1cV0J0g",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 3. Retrieve Relevant Knowledge ---\n",
    "def retrieve_relevant_chunks(query_text, faiss_index, knowledge_chunks_list, embedding_model, top_k=5):\n",
    "    \"\"\"\n",
    "    Embeds the query and retrieves the top_k most relevant chunks from FAISS.\n",
    "    \"\"\"\n",
    "    if not embedding_model or not faiss_index or not knowledge_chunks_list:\n",
    "        print(\"Error: Missing components for retrieval (model, index, or chunks list).\")\n",
    "        return []\n",
    "\n",
    "    print(f\"\\nEmbedding query: \\\"{query_text[:100]}...\\\"\")\n",
    "    query_embedding = embedding_model.encode([query_text], convert_to_tensor=False)\n",
    "    query_embedding_np = np.array(query_embedding, dtype=np.float32)\n",
    "\n",
    "    print(f\"Searching FAISS index for top {top_k} results...\")\n",
    "    distances, indices = faiss_index.search(query_embedding_np, top_k)\n",
    "\n",
    "    retrieved_items = []\n",
    "    print(\"Retrieved results:\")\n",
    "    for i in range(len(indices[0])):\n",
    "        idx = indices[0][i]\n",
    "        dist = distances[0][i]\n",
    "        if 0 <= idx < len(knowledge_chunks_list): # Ensure index is valid\n",
    "            retrieved_chunk_info = knowledge_chunks_list[idx]\n",
    "            retrieved_items.append(retrieved_chunk_info)\n",
    "            source_info = retrieved_chunk_info.get('source_path', retrieved_chunk_info.get('source_file', 'N/A'))\n",
    "            print(f\"  - Index: {idx}, Distance: {dist:.4f}, Source: {source_info}\")\n",
    "        else:\n",
    "            print(f\"  - Warning: Retrieved index {idx} is out of bounds for knowledge_chunks_list (size {len(knowledge_chunks_list)}).\")\n",
    "\n",
    "    return retrieved_items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa13cb7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# --- 4. LLM Interaction (Hugging Face Local Model) ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b3006c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:14:26.785518Z",
     "iopub.status.busy": "2025-05-27T17:14:26.784934Z",
     "iopub.status.idle": "2025-05-27T17:14:26.792047Z",
     "shell.execute_reply": "2025-05-27T17:14:26.791283Z",
     "shell.execute_reply.started": "2025-05-27T17:14:26.785497Z"
    },
    "id": "ybGQStx-0PQ1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 4. LLM Interaction (Hugging Face Local Model - Llama 3 Specific) ---\n",
    "TRUST_REMOTE_CODE_FOR_LLAMA3 = True # Llama 3 often benefits from this, especially for tokenizer features\n",
    "def initialize_local_llm(model_id_to_load=LOCAL_LLM_MODEL_NAME, trust_remote_code=TRUST_REMOTE_CODE_FOR_LLAMA3):\n",
    "    global local_llm_pipeline_instance\n",
    "    \n",
    "    if local_llm_pipeline_instance is not None and local_llm_pipeline_instance != \"error\":\n",
    "        print(f\"Model {model_id_to_load} pipeline is already initialized.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Initializing local LLM pipeline for: {model_id_to_load}\")\n",
    "    try:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Llama 3 is a CausalLM\n",
    "        print(f\"Loading CausalLM model: {model_id_to_load}\")\n",
    "        \n",
    "        # For Llama 3, directly loading the model and tokenizer first can be more stable\n",
    "        # especially if specific tokenizer settings are needed for the chat template.\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id_to_load, trust_remote_code=trust_remote_code)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id_to_load,\n",
    "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\" if device == \"cuda\" else None, # \"auto\" for GPU distribution\n",
    "            trust_remote_code=trust_remote_code\n",
    "        )\n",
    "        \n",
    "        # If device_map=\"auto\" is used, model is already on GPU(s). \n",
    "        # If loading on CPU explicitly, you'd use model.to(device)\n",
    "        # For pipeline, device can be specified too.\n",
    "        \n",
    "        local_llm_pipeline_instance = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model, # Pass loaded model\n",
    "            tokenizer=tokenizer, # Pass loaded tokenizer\n",
    "            # torch_dtype already handled by model loading\n",
    "            # device_map already handled by model loading\n",
    "            device=0 if device == \"cuda\" else -1 # Explicitly tell pipeline primary device\n",
    "        )\n",
    "        \n",
    "        # Llama 3 tokenizer should have pad_token set, but good practice to check\n",
    "        # It's often set to eos_token if not explicitly defined.\n",
    "        if local_llm_pipeline_instance.tokenizer.pad_token_id is None:\n",
    "            if local_llm_pipeline_instance.tokenizer.eos_token_id is not None:\n",
    "                local_llm_pipeline_instance.tokenizer.pad_token_id = local_llm_pipeline_instance.tokenizer.eos_token_id\n",
    "                print(f\"Set tokenizer pad_token_id to eos_token_id: {local_llm_pipeline_instance.tokenizer.eos_token_id}\")\n",
    "            else:\n",
    "                print(\"Warning: eos_token_id is None. Cannot set pad_token_id automatically.\")\n",
    "        print(\"Llama 3 CausalLM pipeline initialized.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing local LLM pipeline for {model_id_to_load}: {e}\")\n",
    "        print(\"Ensure model name is correct, dependencies are installed, you have enough VRAM/RAM, and logged into Hugging Face if needed.\")\n",
    "        local_llm_pipeline_instance = \"error\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b46ed17",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# --- 5. Query LLM with context (Hugging Face Local Model) ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c511d3f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:16:21.520659Z",
     "iopub.status.busy": "2025-05-27T17:16:21.520007Z",
     "iopub.status.idle": "2025-05-27T17:16:21.531706Z",
     "shell.execute_reply": "2025-05-27T17:16:21.530760Z",
     "shell.execute_reply.started": "2025-05-27T17:16:21.520632Z"
    },
    "id": "Zg7gIagB0WY2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 5. Query LLM with context (Hugging Face Local Model - Llama 3 Specific) ---\n",
    "def query_llm_with_context_hf_local(ddr_json_string, retrieved_knowledge_texts):\n",
    "    global local_llm_pipeline_instance, LOCAL_LLM_MODEL_NAME, TRUST_REMOTE_CODE_FOR_LLAMA3\n",
    "    \n",
    "    initialize_local_llm(LOCAL_LLM_MODEL_NAME, TRUST_REMOTE_CODE_FOR_LLAMA3)\n",
    "    if local_llm_pipeline_instance is None or local_llm_pipeline_instance == \"error\":\n",
    "        print(f\"Local LLM pipeline for {LOCAL_LLM_MODEL_NAME} not available.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nConstructing Llama 3 prompt for model: {LOCAL_LLM_MODEL_NAME}...\")\n",
    "    context_str = \"\\\\n\\\\n---\\\\n\\\\n\".join(retrieved_knowledge_texts)\n",
    "    \n",
    "    # System message content for Llama 3\n",
    "    system_message_content = (\n",
    "        \"You are an expert Drilling Risk Analyst. Your task is to analyze the provided Daily Drilling Report (DDR) JSON. \"\n",
    "        \"Use the 'Retrieved Drilling Knowledge Context' to inform your analysis. \"\n",
    "        \"Identify potential risks. For each risk, provide its description, Level (High, Medium, or Low), \"\n",
    "        \"DDR Evidence (JSON path and relevant quote), your Reasoning, and a Solution. \"\n",
    "        \"Your ENTIRE response MUST be a single, valid JSON list of risk objects. \"\n",
    "        \"Do NOT include any explanatory text, introductions, or conclusions before or after the JSON list. \"\n",
    "        \"If no risks are found, your entire response must be ONLY an empty JSON list: `[]`.\"\n",
    "    )\n",
    "\n",
    "    # Few-shot examples for Llama 3\n",
    "    few_shot_example_1_output_str = \"\"\"[\n",
    "  {\n",
    "    \"Risk\": \"Lost Circulation\",\n",
    "    \"Level\": \"Medium\",\n",
    "    \"How Identified\": {\n",
    "      \"DDR Evidence\": \"operations[0].remark: 'Lost 10m3 mud.'\",\n",
    "      \"Reasoning\": \"The DDR states a mud loss of 10m3, indicating potential lost circulation which can affect wellbore stability if not addressed.\"\n",
    "    },\n",
    "    \"How to Avoid this Risk (Solution)\": \"Monitor pit levels closely. If losses occur, spot LCM pills and consider adjusting mud weight if appropriate for the formation.\"\n",
    "  }\n",
    "]\"\"\"\n",
    "    few_shot_example_2_output_str = \"[]\"\n",
    "\n",
    "    user_prompt_content = f\"\"\"\n",
    "    **Daily Drilling Report (JSON):**\n",
    "    ```json\n",
    "    {ddr_json_string}\n",
    "    ```\n",
    "\n",
    "    **Retrieved Drilling Knowledge Context:**\n",
    "    ```\n",
    "    {context_str}\n",
    "    ```\n",
    "\n",
    "    **Your Task:** Based on the DDR and Knowledge Context, identify risks and provide your analysis.\n",
    "    Follow the output format instructions given by the system precisely.\n",
    "\n",
    "    **Here are examples of the exact output format required:**\n",
    "    \n",
    "    *Example 1 (Given a hypothetical simple DDR snippet with a risk):*\n",
    "    Hypothetical DDR: {{\"operations\": [{{\"remark\": \"Lost 10m3 mud.\"}}]}}\n",
    "    Hypothetical Context: \"Mud loss can lead to wellbore instability.\"\n",
    "    Expected Output for Example 1:\n",
    "    ```json\n",
    "    {few_shot_example_1_output_str}\n",
    "    ```\n",
    "\n",
    "    *Example 2 (Given a hypothetical simple DDR with no risks):*\n",
    "    Hypothetical DDR: {{\"operations\": [{{\"remark\": \"Drilling ahead normally.\"}}]}}\n",
    "    Hypothetical Context: \"Normal drilling involves maintaining parameters.\"\n",
    "    Expected Output for Example 2:\n",
    "    ```json\n",
    "    {few_shot_example_2_output_str}\n",
    "    ```\n",
    "    Now, provide your analysis for the given DDR, adhering strictly to the JSON list format for your entire response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Llama 3 Instruct/Chat Template\n",
    "    # The pipeline's tokenizer might also have `apply_chat_template` which is more robust.\n",
    "    # For direct string construction:\n",
    "    messages_for_llama3_template = [\n",
    "        {\"role\": \"system\", \"content\": system_message_content},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_content}\n",
    "        # The pipeline call will implicitly add the assistant cue if tokenizer has a chat template\n",
    "        # Or we can add it manually to the string if needed, but apply_chat_template is better\n",
    "    ]\n",
    "\n",
    "    # Using the tokenizer's chat template is the recommended way for Llama 3\n",
    "    try:\n",
    "        if hasattr(local_llm_pipeline_instance, 'tokenizer') and \\\n",
    "           hasattr(local_llm_pipeline_instance.tokenizer, 'apply_chat_template'):\n",
    "            full_prompt_for_llm = local_llm_pipeline_instance.tokenizer.apply_chat_template(\n",
    "                messages_for_llama3_template,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True # This adds the assistant turn cue, e.g., <|start_header_id|>assistant<|end_header_id|>\n",
    "            )\n",
    "            print(\"Applied Llama 3 chat template via tokenizer.\")\n",
    "        else: # Fallback to manual construction (less ideal but provides an example)\n",
    "            print(\"Warning: Tokenizer does not have apply_chat_template. Using manual Llama 3 prompt construction.\")\n",
    "            full_prompt_for_llm = (\n",
    "                f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "                f\"{system_message_content}<|eot_id|>\"\n",
    "                f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                f\"{user_prompt_content}<|eot_id|>\"\n",
    "                f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\" \n",
    "            )\n",
    "    except Exception as e_template:\n",
    "        print(f\"Error applying chat template, falling back to manual: {e_template}\")\n",
    "        full_prompt_for_llm = (\n",
    "            f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "            f\"{system_message_content}<|eot_id|>\"\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"{user_prompt_content}<|eot_id|>\"\n",
    "            f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\" \n",
    "        )\n",
    "\n",
    "\n",
    "    llm_response_text = None\n",
    "    print(f\"Sending request to Local Hugging Face model: {LOCAL_LLM_MODEL_NAME}...\")\n",
    "    # print(f\"DEBUG: Full prompt for LLM (first 500 chars):\\n{full_prompt_for_llm[:500]}...\")\n",
    "\n",
    "    try:\n",
    "        generation_args = {\n",
    "            \"max_new_tokens\": 2048,\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.1, \n",
    "            \"top_p\": 0.9,\n",
    "            \"eos_token_id\": [local_llm_pipeline_instance.tokenizer.eos_token_id, \n",
    "                             local_llm_pipeline_instance.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")], # Llama3 specific EOT\n",
    "            # \"pad_token_id\" is already set in initialize_local_llm if needed by tokenizer\n",
    "            \"num_return_sequences\": 1,\n",
    "        }\n",
    "        \n",
    "        # The pipeline should handle the prompt correctly if it's a chat prompt string\n",
    "        generated_outputs = local_llm_pipeline_instance(full_prompt_for_llm, **generation_args)\n",
    "        \n",
    "        if generated_outputs and isinstance(generated_outputs, list) and generated_outputs[0]:\n",
    "            llm_response_text = generated_outputs[0]['generated_text']\n",
    "        else:\n",
    "            llm_response_text = \"\"\n",
    "            \n",
    "        # --- Response Cleanup for Llama 3 ---\n",
    "        # The text-generation pipeline often includes the prompt in the output.\n",
    "        # If apply_chat_template was used correctly and add_generation_prompt=True,\n",
    "        # the output should ideally start right after the assistant cue.\n",
    "        \n",
    "        cleaned_response = llm_response_text\n",
    "        \n",
    "        # If the full prompt is echoed:\n",
    "        if llm_response_text.startswith(full_prompt_for_llm.rstrip()):\n",
    "            cleaned_response = llm_response_text[len(full_prompt_for_llm.rstrip()):].lstrip()\n",
    "        else:\n",
    "            # If not an exact match, try to find where the assistant's response starts.\n",
    "            # Llama 3 response starts after \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            assistant_cue = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            cue_index = llm_response_text.rfind(assistant_cue) # Find the last occurrence of the cue\n",
    "            if cue_index != -1:\n",
    "                # Check if this cue was part of our prompt or genuinely start of model output\n",
    "                # This logic can be complex. The pipeline is *supposed* to handle this if chat template applied.\n",
    "                # If we assume the model *only* gives the assistant part after our prompt:\n",
    "                # (This might already be handled if the prompt to pipeline was a list of messages)\n",
    "                # For now, let's assume if the full prompt isn't there, it's mostly the response.\n",
    "                # This part is tricky. `tokenizer.decode(output_ids, skip_special_tokens=True)` after `model.generate` is more robust.\n",
    "                # With pipeline, it's more opaque.\n",
    "                pass # Trusting the pipeline or the simple startswith check for now.\n",
    "\n",
    "        # Remove trailing Llama 3 EOT token if present\n",
    "        if cleaned_response.endswith(\"<|eot_id|>\"):\n",
    "            cleaned_response = cleaned_response[:-len(\"<|eot_id|>\")].strip()\n",
    "        \n",
    "        llm_response_text = cleaned_response\n",
    "        \n",
    "        print(\"Received response from Local LLM.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Local Hugging Face model ({LOCAL_LLM_MODEL_NAME}): {e}\")\n",
    "        return None\n",
    "        \n",
    "    return llm_response_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c150027",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# --- 6. Main RAG Pipeline Function ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3763a62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:16:25.302677Z",
     "iopub.status.busy": "2025-05-27T17:16:25.302405Z",
     "iopub.status.idle": "2025-05-27T17:16:25.313096Z",
     "shell.execute_reply": "2025-05-27T17:16:25.312277Z",
     "shell.execute_reply.started": "2025-05-27T17:16:25.302656Z"
    },
    "id": "Xr0Wpjh-y1BZ",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 6. Main RAG Pipeline Function (Simplified for single model) ---\n",
    "def analyze_ddr_with_rag(parsed_ddr_json, faiss_index, knowledge_chunks_list, query_embedding_model):\n",
    "    if not parsed_ddr_json:\n",
    "        print(\"No parsed DDR data provided.\")\n",
    "        return None\n",
    "\n",
    "    ddr_json_string_for_prompt = json.dumps(parsed_ddr_json, indent=2, ensure_ascii=False)\n",
    "    query_parts = []\n",
    "\n",
    "    summary = parsed_ddr_json.get(\"summary_of_activities_24_hours\", \"\")\n",
    "    if summary:\n",
    "        query_parts.append(f\"Summary of activities: {summary}\")\n",
    "    \n",
    "    operations = parsed_ddr_json.get(\"operations\", [])\n",
    "    if operations:\n",
    "        problematic_remarks = [op.get(\"remark\", \"\") for op in operations if op.get(\"remark\") and \n",
    "                               any(kw in op.get(\"remark\", \"\").lower() for kw in \n",
    "                                   [\"tight\", \"stuck\", \"gain\", \"loss\", \"fail\", \"overpull\", \"jar\", \"issue\", \"problem\", \"leak\", \"noise\", \"unable\"])]\n",
    "        if problematic_remarks:\n",
    "             query_parts.append(f\"Key operational issues/remarks: {'; '.join(problematic_remarks)}\")\n",
    "        elif len(operations) > 0:\n",
    "            last_ops_summary = \"; \".join([op.get(\"main_sub_activity\",\"\") + \": \" + op.get(\"remark\",\"\") for op in operations[-3:]])\n",
    "            query_parts.append(f\"Recent operations: {last_ops_summary}\")\n",
    "\n",
    "    if not query_parts:\n",
    "        query_parts.append(f\"General drilling risks analysis for well {parsed_ddr_json.get('wellbore_name', 'Unknown Well')}\")\n",
    "\n",
    "    composite_query = \". \".join(query_parts)\n",
    "    \n",
    "    retrieved_knowledge_items = retrieve_relevant_chunks(\n",
    "        composite_query, faiss_index, knowledge_chunks_list, \n",
    "        query_embedding_model, top_k=7 # You can adjust top_k\n",
    "    )\n",
    "    \n",
    "    retrieved_texts_for_llm = [item['text'] for item in retrieved_knowledge_items]\n",
    "    if not retrieved_texts_for_llm:\n",
    "        print(\"No relevant knowledge chunks retrieved. LLM will rely solely on DDR content.\")\n",
    "\n",
    "    # Call the Llama 3 specific query function\n",
    "    llm_analysis_str = query_llm_with_context_hf_local(\n",
    "        ddr_json_string_for_prompt, \n",
    "        retrieved_texts_for_llm\n",
    "    )\n",
    "\n",
    "    # --- JSON Parsing Logic (Keep your improved version from previous step) ---\n",
    "    if llm_analysis_str:\n",
    "        print(\"\\\\n--- LLM Analysis (Raw String) ---\")\n",
    "        print(llm_analysis_str)\n",
    "        \n",
    "        json_to_parse = \"\"\n",
    "        # Try to find content within ```json ... ``` first\n",
    "        json_match = re.search(r\"```json\\s*([\\s\\S]*?)\\s*```\", llm_analysis_str, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_to_parse = json_match.group(1).strip()\n",
    "            print(\"DEBUG: Extracted JSON string from markdown code block.\")\n",
    "        else:\n",
    "            # If no markdown block, try to find a raw JSON list\n",
    "            list_match = re.search(r\"(\\[\\]|\\[\\s*\\{[\\s\\S]*?\\}\\s*\\])\", llm_analysis_str, re.DOTALL)\n",
    "            if list_match:\n",
    "                json_to_parse = list_match.group(0).strip()\n",
    "                print(\"DEBUG: Found raw JSON list string.\")\n",
    "            else:\n",
    "                print(\"Could not find a JSON markdown block or a raw JSON list in the LLM response.\")\n",
    "                return {\"error\": \"LLM output did not contain a findable JSON structure\", \"raw_output\": llm_analysis_str}\n",
    "\n",
    "        if not json_to_parse:\n",
    "            print(\"JSON string to parse is empty after extraction attempts.\")\n",
    "            return {\"error\": \"Extracted JSON string is empty\", \"raw_output\": llm_analysis_str}\n",
    "            \n",
    "        try:\n",
    "            print(f\"DEBUG: Attempting to parse: -->{json_to_parse}<--\")\n",
    "            parsed_llm_output = json.loads(json_to_parse)\n",
    "            return parsed_llm_output\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding extracted JSON string: {e}\")\n",
    "            print(f\"Extracted string that failed parsing was: -->{json_to_parse}<--\")\n",
    "            return {\"error\": \"Extracted string was not valid JSON\", \"raw_output\": llm_analysis_str, \"extracted_for_parse\": json_to_parse}\n",
    "    else:\n",
    "        print(\"No analysis received from LLM.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e09614",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# --- Main Execution Example ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b56af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T17:21:09.209957Z",
     "iopub.status.busy": "2025-05-27T17:21:09.209331Z",
     "iopub.status.idle": "2025-05-27T17:21:35.143462Z",
     "shell.execute_reply": "2025-05-27T17:21:35.142551Z",
     "shell.execute_reply.started": "2025-05-27T17:21:09.209933Z"
    },
    "id": "tVS9Kauu0fOF",
    "outputId": "9b2c47c9-2fa8-43f5-dcde-2da13a353bff",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Main Execution Example (Simplified for single Llama 3 model) ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Using PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "\n",
    "    faiss_index, knowledge_chunks_list = load_indexed_knowledge(FAISS_INDEX_PATH, CHUNKS_JSON_PATH)\n",
    "    query_embedding_model = initialize_embedding_model(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "    if not faiss_index or not knowledge_chunks_list or not query_embedding_model:\n",
    "        print(\"Failed to load necessary components for RAG. Exiting.\")\n",
    "    else:\n",
    "        # Use your specific parsed DDR file path\n",
    "        example_ddr_filepath = \"/kaggle/input/modeling/parsed_ddr_example.json\" # Or your new parsed file path\n",
    "        \n",
    "        # Create a dummy DDR if the primary one is not found, for basic testing\n",
    "        if not os.path.exists(example_ddr_filepath):\n",
    "            print(f\"Warning: Specified DDR file '{example_ddr_filepath}' not found. Using a built-in dummy DDR for demonstration.\")\n",
    "            # parsed_ddr_to_analyze = {\n",
    "            #     \"wellbore_name\": \"DUMMY_FIXED_002\",\n",
    "            #     \"period\": \"2024-01-03 to 2024-01-04\",\n",
    "            #     \"summary_of_activities_24_hours\": \"Drilled 8.5in hole. Experienced severe losses at 3500m. Pumped LCM. Torque fluctuating.\",\n",
    "            #     \"operations\": [\n",
    "            #         {\"start_time\": \"10:00\", \"end_time\": \"15:00\", \"end_depth_mmd\": \"3500\", \"main_sub_activity\": \"drilling -- drill\", \"state\": \"problem\", \"remark\": \"Severe mud losses (100 m3/hr) encountered at 3500m. Stopped drilling.\"},\n",
    "            #         {\"start_time\": \"15:00\", \"end_time\": \"18:00\", \"end_depth_mmd\": \"3500\", \"main_sub_activity\": \"circulation -- lost_circulation_material\", \"state\": \"remedial\", \"remark\": \"Pumped 3 x 20 m3 high concentration LCM pills. Losses reduced to 5 m3/hr.\"},\n",
    "            #         {\"start_time\": \"18:00\", \"end_time\": \"19:00\", \"end_depth_mmd\": \"3500\", \"main_sub_activity\": \"drilling -- ream\", \"state\": \"problem\", \"remark\": \"Attempted to ream section, torque fluctuating between 15-35 kNm.\"}\n",
    "            #     ],\n",
    "            #     \"gas_reading_information\": [],\n",
    "            #     \"drilling_fluid\": [{\"sample_time\": \"09:00\", \"fluid_density_g_cm3\": \"1.55\"}]\n",
    "            # }\n",
    "        else:\n",
    "            try:\n",
    "                with open(example_ddr_filepath, 'r', encoding='utf-8') as f:\n",
    "                    parsed_ddr_to_analyze = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading parsed DDR example from {example_ddr_filepath}: {e}\")\n",
    "                parsed_ddr_to_analyze = None # Ensure it's None if loading fails\n",
    "        \n",
    "        if parsed_ddr_to_analyze:\n",
    "            print(f\"\\n--- Analyzing DDR for Well: {parsed_ddr_to_analyze.get('wellbore_name', 'N/A')} using {LOCAL_LLM_MODEL_NAME} ---\")\n",
    "            \n",
    "            risk_analysis_results = analyze_ddr_with_rag(\n",
    "                parsed_ddr_to_analyze, \n",
    "                faiss_index, \n",
    "                knowledge_chunks_list, \n",
    "                query_embedding_model\n",
    "            )\n",
    "\n",
    "            print(f\"\\n--- Final Structured Risk Analysis from {LOCAL_LLM_MODEL_NAME} ---\")\n",
    "            if risk_analysis_results:\n",
    "                if isinstance(risk_analysis_results, dict) and \"error\" in risk_analysis_results:\n",
    "                    print(f\"Error in analysis: {risk_analysis_results['error']}\")\n",
    "                    print(f\"Raw LLM Output (if any):\\n{risk_analysis_results.get('raw_output', 'N/A')}\")\n",
    "                    if \"extracted_for_parse\" in risk_analysis_results:\n",
    "                        print(f\"String that failed JSON parsing:\\n-->{risk_analysis_results['extracted_for_parse']}<--\")\n",
    "                else:\n",
    "                    print(json.dumps(risk_analysis_results, indent=2, ensure_ascii=False))\n",
    "            else:\n",
    "                print(f\"\\nNo structured risk analysis was produced by {LOCAL_LLM_MODEL_NAME}.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7529893,
     "sourceId": 11973921,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 55797,
     "modelInstanceId": 39831,
     "sourceId": 47572,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1447.437247,
   "end_time": "2025-06-09T15:51:55.465577",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-09T15:27:48.028330",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
